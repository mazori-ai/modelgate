# ModelGate Open Source Edition - Configuration
# =============================================================================
# Provider API keys are configured in the Dashboard UI under Providers section.
# This file contains server settings and model definitions.
# =============================================================================

[server]
http_port = 8080         # Unified API: OpenAI (/v1/*), GraphQL (/graphql), MCP (/mcp)
bind_address = "0.0.0.0"
read_timeout = "30s"
write_timeout = "30s"
# auth_token = "${MODELGATE_AUTH_TOKEN}"  # Uncomment to require authentication

# Adaptive dispatcher configuration (Bifrost-style)
min_workers = 5                # Minimum workers (always running)
max_workers = 200              # Maximum workers (auto-scale limit)
max_queued_requests = 1000     # Max requests waiting in queue
scale_up_threshold = 0.7       # Scale up when queue > 70% full
scale_down_threshold = 0.2     # Scale down when queue < 20% full

# =============================================================================
# Embedder Configuration for Semantic Tool Search
# =============================================================================
# Used by MCP Gateway for semantic search of tools
# Options: "openai", "ollama" (default: ollama)
# Used for: MCP semantic tool search AND semantic response caching
# =============================================================================

[embedder]
type = "ollama"                          # "openai" or "ollama" (default: ollama)
base_url = "http://localhost:11434"      # Ollama server URL (default: http://localhost:11434)
model = "nomic-embed-text"               # Default: nomic-embed-text (Ollama), text-embedding-3-small (OpenAI)
# api_key = ""                           # Required only for OpenAI

# =============================================================================
# Database Configuration
# =============================================================================

[database]
driver = "postgres"
host = "localhost"
port = 5432
user = "postgres"
password = "postgres"
database = "modelgate"    # Single database for all data
ssl_mode = "disable"
max_conns = 20
max_idle = 5
conn_max_age = "30m"

[telemetry]
enabled = true
prometheus_enabled = true
prometheus_port = 9090
log_level = "debug"
# otlp_endpoint = "http://localhost:4317"  # Uncomment for OpenTelemetry export

# =============================================================================
# Model Aliases
# =============================================================================
# Map friendly names to full model identifiers

[aliases]
default = "anthropic/claude-sonnet-4-20250514"
claude = "anthropic/claude-sonnet-4-20250514"
claude-opus = "anthropic/claude-opus-4-20250514"
claude-sonnet = "anthropic/claude-sonnet-4-20250514"
claude-haiku = "anthropic/claude-3-5-haiku-20241022"
gemini = "gemini/gemini-2.5-pro-preview-06-05"
gemini-pro = "gemini/gemini-2.5-pro-preview-06-05"
gemini-flash = "gemini/gemini-2.5-flash-preview-05-20"
gpt4 = "openai/gpt-4o"
gpt4o = "openai/gpt-4o"
gpt4-mini = "openai/gpt-4o-mini"

# =============================================================================
# MODEL DEFINITIONS
# =============================================================================
# Model metadata (capabilities, pricing) used for analytics and billing.
# To use a model:
# 1. Configure the provider API key in the Dashboard UI
# 2. Enable the model in the Models page
# =============================================================================

# =============================================================================
# Anthropic Models
# =============================================================================

[models."anthropic/claude-opus-4-20250514"]
name = "Claude Opus 4"
provider = "anthropic"
supports_tools = true
supports_reasoning = true
context_limit = 200000
output_limit = 32000
input_cost_per_1m = 15.0
output_cost_per_1m = 75.0

[models."anthropic/claude-sonnet-4-20250514"]
name = "Claude Sonnet 4"
provider = "anthropic"
supports_tools = true
supports_reasoning = true
context_limit = 200000
output_limit = 64000
input_cost_per_1m = 3.0
output_cost_per_1m = 15.0

[models."anthropic/claude-3-5-haiku-20241022"]
name = "Claude 3.5 Haiku"
provider = "anthropic"
supports_tools = true
supports_reasoning = false
context_limit = 200000
output_limit = 8192
input_cost_per_1m = 0.8
output_cost_per_1m = 4.0

# =============================================================================
# Google Gemini Models
# =============================================================================

[models."gemini/gemini-2.5-pro-preview-06-05"]
name = "Gemini 2.5 Pro"
provider = "gemini"
supports_tools = true
supports_reasoning = true
context_limit = 1000000
output_limit = 65536
input_cost_per_1m = 2.5
output_cost_per_1m = 15.0

[models."gemini/gemini-2.5-flash-preview-05-20"]
name = "Gemini 2.5 Flash"
provider = "gemini"
supports_tools = true
supports_reasoning = true
context_limit = 1000000
output_limit = 65536
input_cost_per_1m = 0.15
output_cost_per_1m = 0.60

[models."gemini/gemini-2.0-flash"]
name = "Gemini 2.0 Flash"
provider = "gemini"
supports_tools = true
supports_reasoning = false
context_limit = 1000000
output_limit = 8192
input_cost_per_1m = 0.10
output_cost_per_1m = 0.40

# =============================================================================
# OpenAI Models
# =============================================================================

[models."openai/gpt-4o"]
name = "GPT-4o"
provider = "openai"
supports_tools = true
supports_reasoning = false
context_limit = 128000
output_limit = 16384
input_cost_per_1m = 2.50
output_cost_per_1m = 10.0

[models."openai/gpt-4o-mini"]
name = "GPT-4o Mini"
provider = "openai"
supports_tools = true
supports_reasoning = false
context_limit = 128000
output_limit = 16384
input_cost_per_1m = 0.15
output_cost_per_1m = 0.60

[models."openai/o1"]
name = "OpenAI O1"
provider = "openai"
supports_tools = true
supports_reasoning = true
context_limit = 200000
output_limit = 100000
input_cost_per_1m = 15.0
output_cost_per_1m = 60.0

[models."openai/o3"]
name = "OpenAI O3"
provider = "openai"
supports_tools = true
supports_reasoning = true
context_limit = 200000
output_limit = 100000
input_cost_per_1m = 2.0
output_cost_per_1m = 8.0

# =============================================================================
# AWS Bedrock Models
# =============================================================================

[models."bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0"]
name = "Claude 3.5 Sonnet v2 (Bedrock)"
provider = "bedrock"
supports_tools = true
supports_reasoning = false
context_limit = 200000
output_limit = 8192
input_cost_per_1m = 3.0
output_cost_per_1m = 15.0

[models."bedrock/amazon.nova-pro-v1:0"]
name = "Amazon Nova Pro"
provider = "bedrock"
supports_tools = true
supports_reasoning = false
context_limit = 300000
output_limit = 5000
input_cost_per_1m = 0.80
output_cost_per_1m = 3.20

# =============================================================================
# Ollama Models (Local)
# =============================================================================

[models."ollama/llama3.2"]
name = "Llama 3.2"
provider = "ollama"
supports_tools = false
supports_reasoning = false
context_limit = 128000
output_limit = 4096
input_cost_per_1m = 0.0
output_cost_per_1m = 0.0

[models."ollama/qwen3"]
name = "Qwen 3"
provider = "ollama"
supports_tools = true
supports_reasoning = false
context_limit = 40960
output_limit = 4096
input_cost_per_1m = 0.0
output_cost_per_1m = 0.0

# =============================================================================
# Embedding Models
# =============================================================================

[models."openai/text-embedding-3-small"]
name = "OpenAI Embedding Small"
provider = "openai"
supports_tools = false
supports_reasoning = false
context_limit = 8191
output_limit = 0
input_cost_per_1m = 0.02
output_cost_per_1m = 0.0

[models."gemini/text-embedding-004"]
name = "Gemini Embedding"
provider = "gemini"
supports_tools = false
supports_reasoning = false
context_limit = 2048
output_limit = 0
input_cost_per_1m = 0.00
output_cost_per_1m = 0.0

