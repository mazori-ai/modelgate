# ModelGate Open Source Edition - Configuration
# =============================================================================
# Provider API keys and models are configured in the Dashboard UI.
# This file contains server settings only.
# =============================================================================

[server]
http_port = 8080         # Unified API: OpenAI (/v1/*), GraphQL (/graphql), MCP (/mcp)
bind_address = "0.0.0.0"
read_timeout = "30s"
write_timeout = "30s"

# Adaptive dispatcher configuration
min_workers = 5                # Minimum workers (always running)
max_workers = 200              # Maximum workers (auto-scale limit)
max_queued_requests = 1000     # Max requests waiting in queue
scale_up_threshold = 0.7       # Scale up when queue > 70% full
scale_down_threshold = 0.2     # Scale down when queue < 20% full

# =============================================================================
# Database Configuration
# =============================================================================

[database]
driver = "postgres"
host = "localhost"
port = 5432
user = "postgres"
password = "postgres"
database = "modelgate"
ssl_mode = "disable"
max_conns = 20
max_idle = 5
conn_max_age = "30m"

# =============================================================================
# Embedder Configuration for Semantic Features
# =============================================================================
# Used for: MCP semantic tool search AND semantic response caching
# Options: "ollama" (default, local) or "openai" (cloud)
# =============================================================================

[embedder]
type = "ollama"                          # "openai" or "ollama"
base_url = "http://localhost:11434"      # Ollama server URL
model = "nomic-embed-text"               # Embedding model
# api_key = ""                           # Required only for OpenAI

# =============================================================================
# Telemetry Configuration
# =============================================================================

[telemetry]
enabled = true
prometheus_enabled = true
prometheus_port = 9090
log_level = "info"                       # debug, info, warn, error
# otlp_endpoint = "http://localhost:4317"  # OpenTelemetry export (optional)

# =============================================================================
# Model Aliases (Optional)
# =============================================================================
# Map friendly names to full model identifiers for convenience.
# Example: Use "claude" instead of "anthropic/claude-sonnet-4-20250514"
# =============================================================================

[aliases]
# Uncomment and customize as needed:
# default = "anthropic/claude-sonnet-4-20250514"
# claude = "anthropic/claude-sonnet-4-20250514"
# gpt4 = "openai/gpt-4o"
# gemini = "gemini/gemini-2.0-flash"
